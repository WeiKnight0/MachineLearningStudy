{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bfb1105f73e145d",
   "metadata": {},
   "source": [
    "# 线性回归(Linear Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79030d21842eafb",
   "metadata": {},
   "source": [
    "## 基本概念\n",
    "- 线性回归（Linear Regression）：用于预测连续数值型变量的监督学习算法\n",
    "- 目标：找到输入变量与输出变量之间的线性关系\n",
    "- 单变量线性回归模型的数学表达式：\n",
    "$$\n",
    "    y =f_{w,b}(x) = w \\cdot x + b\n",
    "$$\n",
    "- 其中，y是预测值，x_i是输入特征，w_i是对应的权重，w_0是偏置项\n",
    "- 线性回归的术语：\n",
    "    - 输入$x$：特征向量\n",
    "    - 输出$y$：目标变量\n",
    "    - $m$：训练样例数量\n",
    "    - $x^{(i)}$：第$i$个训练样例的特征向量\n",
    "    - $y^{(i)}$：第$i$个训练样例的目标变量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155566f23ced9b",
   "metadata": {},
   "source": [
    "## 成本函数\n",
    "- 成本函数（Cost Function）：衡量模型预测值与真实值之间差异的函数\n",
    "- 线性回归中常用的成本函数是均方误差（Mean Squared Error, MSE）：\n",
    "$$\n",
    "    J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "- 目标：最小化成本函数，找到最佳的权重和偏置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558484c89fee0380",
   "metadata": {},
   "source": [
    "## 可视化的成本函数\n",
    "例：\n",
    "训练集：\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 2 |\n",
    "| 2 | 3 |\n",
    "| 3 | 5 |\n",
    "\n",
    "假设模型：$\\hat{y} = wx + b$。\n",
    "如果我们选择 $w=1$ 和 $b=1$，则成本函数计算如下：\n",
    "$$\n",
    "    J(1, 1) = \\frac{1}{2 \\times 3} \\left[ (1 \\times 1 + 1 - 2)^2 + (1 \\times 2 + 1 - 3)^2 + (1 \\times 3 + 1 - 5)^2 \\right] = \\frac{1}{6} [ 0^2 + 0^2 + (-1)^2 ] = \\frac{1}{6} \\approx 0.1667\n",
    "$$\n",
    "对于不同的 $w$ 和 $b$ 组合，我们可以计算出不同的成本值，并绘制出成本函数的三维图像，展示成本函数如何随着 $w$ 和 $b$ 的变化而变化。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1d5613546b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练数据\n",
    "X = np.array([1, 2, 3])\n",
    "Y = np.array([2, 3, 5])\n",
    "m = len(Y)\n",
    "\n",
    "# 计算成本函数\n",
    "def compute_cost(w, b):\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        y_pred = w * X[i] + b\n",
    "        total_cost += (y_pred - Y[i]) ** 2\n",
    "    return total_cost / (2 * m)\n",
    "\n",
    "# 生成 w 和 b 的值\n",
    "w_values = np.linspace(-100, 100, 1000)\n",
    "b_values = np.linspace(-100, 100, 1000)\n",
    "W, B = np.meshgrid(w_values, b_values)\n",
    "J = np.zeros(W.shape)\n",
    "\n",
    "# 计算每个 (w, b) 对应的成本\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        J[i, j] = compute_cost(W[i, j], B[i, j])\n",
    "\n",
    "# 绘制成本函数的三维图像\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W, B, J, cmap='viridis')\n",
    "ax.set_xlabel('Weight (w)')\n",
    "ax.set_ylabel('Bias (b)')\n",
    "ax.set_zlabel('Cost J(w, b)')\n",
    "ax.set_title('Cost Function Surface')\n",
    "plt.show()\n",
    "\n",
    "# 绘制等高线图\n",
    "plt.contour(W, B, J, levels=50, cmap='viridis')\n",
    "plt.xlabel('Weight (w)')\n",
    "plt.ylabel('Bias (b)')\n",
    "plt.title('Cost Function Contour')\n",
    "plt.colorbar(label='Cost J(w, b)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869da889a5e76d64",
   "metadata": {},
   "source": [
    "## 梯度下降法\n",
    "- 梯度下降法（Gradient Descent）：一种优化算法，用于最小化成本函数\n",
    "- 基本思想：沿着成本函数的负梯度方向更新参数，从而不断减小成本函数，直到收敛到局部最小值。\n",
    "- 参数更新公式：\n",
    "$$\n",
    "   w = w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "    b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\n",
    "$$\n",
    "- 其中，$\\alpha$是学习率，控制每次更新的步长\n",
    "- 迭代过程：\n",
    "    1. 初始化参数$w$和$b$\n",
    "    2. 计算成本函数$J(w, b)$\n",
    "    3. 计算梯度$\\frac{\\partial J(w, b)}{\\partial w_j}$和$\\frac{\\partial J(w, b)}{\\partial b}$\n",
    "    4. 更新参数$mathbf{w}$和$b$\n",
    "    5. 重复步骤2-4，直到收敛\n",
    "\n",
    "注意！一定要先计算微分，再更新参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fff76c3846a33",
   "metadata": {},
   "source": [
    "## 学习率\n",
    "- 学习率（Learning Rate, $\\alpha$）：控制参数更新步长的超参数\n",
    "- 选择合适的学习率非常重要：\n",
    "    - 学习率过大：可能导致参数更新过快，错过最优解，甚至发散\n",
    "    - 学习率过小：收敛速度过慢\n",
    "- 当使用固定学习率时，往往能达到局部最小值。这是因为当参数接近最优值时，梯度变得很小，导致参数更新幅度减小，从而实现收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5f474d89f9750",
   "metadata": {},
   "source": [
    "## 线性回归的梯度下降\n",
    "线性回归中：\n",
    "$$\n",
    "    y = f_{w, b}(x) = w \\cdot x + b\n",
    "$$\n",
    "成本函数：\n",
    "$$\n",
    "    J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "对成本函数求偏导数：\n",
    "$$\n",
    "    \\frac{\\partial J(w, b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)}) x^{(i)}\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial J(w, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "参数更新公式：\n",
    "$$\n",
    "    w := w - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)}) x^{(i)}\n",
    "$$\n",
    "$$\n",
    "    b := b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{w, b}(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "\n",
    "由于线性回归的成本函数是一个凸函数，使用梯度下降法可以保证收敛到**全局最小值**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78280b8912fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归的梯度下降实现\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练数据\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([2, 3, 5, 7, 11])\n",
    "m = len(Y)\n",
    "\n",
    "# 计算成本函数\n",
    "def compute_cost(w, b):\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        y_pred = np.dot(w, X[i]) + b\n",
    "        total_cost += (y_pred - Y[i]) ** 2\n",
    "    return total_cost / (2 * m)\n",
    "\n",
    "# 梯度下降算法\n",
    "def gradient_descent(X, Y, alpha, num_iterations):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "    for _ in range(num_iterations):\n",
    "        dw = np.zeros(X.shape[1])\n",
    "        db = 0\n",
    "        for i in range(m):\n",
    "            y_pred = np.dot(w, X[i]) + b\n",
    "            error = y_pred - Y[i]\n",
    "            dw += error * X[i]\n",
    "            db += error\n",
    "        w -= alpha * (dw / m)\n",
    "        b -= alpha * (db / m)\n",
    "        cost = compute_cost(w, b)\n",
    "        cost_history.append(cost)\n",
    "    return w, b, cost_history\n",
    "\n",
    "# 训练模型\n",
    "alpha = 0.01\n",
    "num_iterations = 100\n",
    "w, b, cost_history = gradient_descent(X, Y, alpha, num_iterations)\n",
    "print(f\"Learned parameters: w = {w}, b = {b}\")\n",
    "\n",
    "# 绘制成本函数的变化\n",
    "plt.plot(range(num_iterations), cost_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost J(w, b)')\n",
    "plt.title('Cost Function Convergence')\n",
    "plt.show()\n",
    "\n",
    "# 绘制拟合结果\n",
    "plt.scatter(X, Y, color='red', label='Training Data')\n",
    "plt.plot(X, np.dot(X, w) + b, color='blue', label='Fitted Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8aa6472fcfc3bd",
   "metadata": {},
   "source": [
    "## 多特征\n",
    "- 多特征线性回归（Multi-Features Linear Regression）：处理多个输入特征的线性回归问题\n",
    "- 符号\n",
    "    - 输入特征向量：$\\mathbf{x} = [x_1, x_2, ..., x_n]^\\mathsf{T}$\n",
    "        - 其中，$n$是特征数量\n",
    "        - $x_j$是第$j$个特征\n",
    "        - 则第$i$个训练样例的第$j$个特征表示为$x_j^{(i)}$\n",
    "    - 权重向量：$\\mathbf{w} = [w_1, w_2, ..., w_n]^\\mathsf{T}$\n",
    "        - $w_j$是第$j$个特征对应的权重\n",
    "- 模型表达式：\n",
    "$$\n",
    "    y = f_{\\mathbf{w}, b}(\\mathbf{x}) = \\mathbf{w}^\\mathsf{T}\\mathbf{x} +b = w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_n \\cdot x_n + b\n",
    "$$\n",
    "- 其中，$\\mathbf{x} = [x_1, x_2, ..., x_n]^\\mathsf{T}$是输入特征向量，$\\mathbf{w} = [w_1, w_2, ..., w_n]^\\mathsf{T}$是对应的权重向量\n",
    "- $\\mathbf{w}^\\mathsf{T}\\mathbf{x}$表示权重向量与特征向量的点积。向量化操作可以使用`numpy`中的`dot`函数实现，效率更高。\n",
    "```python\n",
    "f = np.dot(w, x) + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a048a262568c9",
   "metadata": {},
   "source": [
    "## 多特征的梯度下降\n",
    "- 成本函数：\n",
    "$$\n",
    "    J(\\mathbf{w}, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "- 对成本函数求偏导数：\n",
    "$$\n",
    "    \\frac{\\partial J(\\mathbf{w}, b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)})\n",
    "$$\n",
    "- 参数更新公式：\n",
    "$$\n",
    "    w_j := w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    b := b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)})\n",
    "$$\n",
    "\n",
    "- 向量化的参数更新公式：\n",
    "$$\n",
    "\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    b := b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)})\n",
    "$$\n",
    "- 向量化的梯度计算和参数更新可以使用`numpy`中的矩阵运算实现，效率更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06718265ac80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练数据（多特征）\n",
    "X = np.array([[1, 2], [2, 3], [3, 5], [4, 7], [5, 11]])\n",
    "Y = np.array([2, 3, 5, 7, 11])\n",
    "m = len(Y)\n",
    "\n",
    "# 计算成本函数\n",
    "def compute_cost(w, b):\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        y_pred = np.dot(w, X[i]) + b\n",
    "        total_cost += (y_pred - Y[i]) ** 2\n",
    "    return total_cost / (2 * m)\n",
    "\n",
    "# 梯度下降算法（向量化实现）\n",
    "def gradient_descent(X, Y, alpha, num_iterations):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "    for _ in range(num_iterations):\n",
    "        y_preds = np.dot(X, w) + b\n",
    "        errors = y_preds - Y\n",
    "        dw = np.dot(errors, X) / m\n",
    "        db = np.sum(errors) / m\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        cost = compute_cost(w, b)\n",
    "        cost_history.append(cost)\n",
    "    return w, b, cost_history\n",
    "\n",
    "# 训练模型\n",
    "alpha = 0.01\n",
    "num_iterations = 100\n",
    "w, b, cost_history = gradient_descent(X, Y, alpha, num_iterations)\n",
    "print(f\"Learned parameters: w = {w}, b = {b}\")\n",
    "# 绘制成本函数的变化\n",
    "plt.plot(range(num_iterations), cost_history)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost J(w, b)')\n",
    "plt.title('Cost Function Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884efad3980ed3f",
   "metadata": {},
   "source": [
    "## 特征缩放\n",
    "- 特征缩放（Feature Scaling）：将不同尺度的特征转换到相似的尺度范围内，以提高梯度下降的收敛速度\n",
    "- 例如：\n",
    "- 原始特征：\n",
    "    - 特征1（年龄）：[18, 25, 30, 45, 60]\n",
    "    - 特征2（收入）：[20000, 50000, 80000, 120000, 200000]\n",
    "    - 发现特征2的数值范围远大于特征1，可能导致梯度下降收敛缓慢\n",
    "- 常见的特征缩放方法：\n",
    "    - 最小-最大缩放：将特征值缩放到[0, 1]范围内\n",
    "    $$\n",
    "        x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n",
    "    $$\n",
    "    - 均值缩放（Mean Normalization）：将特征值缩放到[-1, 1]范围内\n",
    "    $$\n",
    "        x' = \\frac{x - \\mu}{x_{max} - x_{min}}\n",
    "    $$\n",
    "    - Z-score标准化（Z-score Normalization）：将特征值转换为均值为0，标准差为1的高斯分布\n",
    "    $$\n",
    "        x' = \\frac{x - \\mu}{\\sigma}\n",
    "    $$\n",
    "    - 其中，$x_{min}$和$x_{max}$分别是特征的最小值和最大值，$\\mu$是特征的均值，$\\sigma$是特征的标准差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c2d674a446b21",
   "metadata": {},
   "source": [
    "## 检查梯度下降是否收敛\n",
    "- 学习曲线（Learning Curve）：绘制成本函数值随迭代次数变化的曲线\n",
    "    - 如果成本函数值逐渐减小并趋于平稳，说明梯度下降收敛\n",
    "    - 如果成本函数值波动较大或不减小，可能需要调整学习率或检查实现\n",
    "- 可以设置一个阈值$\\varepsilon$，当连续多次迭代成本函数值的变化小于该阈值时，认为梯度下降已经收敛\n",
    "    - 例如，当连续10次迭代中成本函数值的变化小于0.0001时，认为收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b8be29b576a8d",
   "metadata": {},
   "source": [
    "## 选择学习率\n",
    "- 学习率选择技巧：\n",
    "    - 经验法则：从一个较小的学习率开始（如0.01），观察成本函数的变化\n",
    "    - 学习率调度（Learning Rate Scheduling）：在训练过程中动态调整学习率\n",
    "        - 例如，随着迭代次数增加，逐渐减小学习率\n",
    "    - 使用自适应学习率算法，如Adam、RMSprop等，这些算法会根据梯度信息自动调整学习率\n",
    "- 当学习率太小时：收敛速度慢\n",
    "- 当学习率太大时：可能导致学习曲线波动幅度大，甚至发散不收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc8dbc5c759675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：不同学习率下的梯度下降效果比较\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练数据\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "Y = np.array([2, 3, 5, 7, 11])\n",
    "m = len(Y)\n",
    "\n",
    "# 计算成本函数\n",
    "def compute_cost(w, b):\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        y_pred = np.dot(w, X[i]) + b\n",
    "        total_cost += (y_pred - Y[i]) ** 2\n",
    "    return total_cost / (2 * m)\n",
    "\n",
    "# 梯度下降算法\n",
    "def gradient_descent(X, Y, alpha, num_iterations):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "    for _ in range(num_iterations):\n",
    "        dw = np.zeros(X.shape[1])\n",
    "        db = 0\n",
    "        for i in range(m):\n",
    "            y_pred = np.dot(w, X[i]) + b\n",
    "            error = y_pred - Y[i]\n",
    "            dw += error * X[i]\n",
    "            db += error\n",
    "        w -= alpha * (dw / m)\n",
    "        b -= alpha * (db / m)\n",
    "        cost = compute_cost(w, b)\n",
    "        cost_history.append(cost)\n",
    "    return w, b, cost_history\n",
    "\n",
    "# 不同学习率下的训练\n",
    "learning_rates = [0.001, 0.01, 0.17]\n",
    "num_iterations = 100\n",
    "plt.figure(figsize=(12, 8))\n",
    "for alpha in learning_rates:\n",
    "    w, b, cost_history = gradient_descent(X, Y, alpha, num_iterations)\n",
    "    plt.plot(range(num_iterations), cost_history, label=f'Learning Rate: {alpha}')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost J(w, b)')\n",
    "plt.title('Cost Function Convergence for Different Learning Rates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb858e66ae7d2585",
   "metadata": {},
   "source": [
    "## 多项式回归\n",
    "- 多项式回归（Polynomial Regression）：线性回归的一种扩展，用于拟合非线性关系\n",
    "- 通过引入输入特征的高次项，将非线性关系转换为线性关系\n",
    "- 例如，二次多项式回归模型的数学表达式：\n",
    "$$\n",
    "    y = f_{w, b}(x) = w_2 \\cdot x^2 + w_1 \\cdot x + b\n",
    "$$\n",
    "- 例如，含根号的多项式回归模型：\n",
    "$$\n",
    "    y = f_{w, b}(x) = w_2 \\cdot \\sqrt{x} + w_1 \\cdot x + b\n",
    "$$\n",
    "- 多项式回归的训练过程与线性回归类似，仍然使用梯度下降法最小化成本函数\n",
    "- 需要注意的是，多项式回归容易导致过拟合问题，因此在选择多项式的阶数时需要谨慎\n",
    "- 可以使用`scikit-learn`库中的`PolynomialFeatures`类来生成多项式特征"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
