{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 神经网络 Neural Networks"
   ],
   "id": "ca553079a3d06639"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 神经网路与大脑\n",
    "- 神经网络（Neural Networks）是一种受生物神经系统启发的计算模型。\n",
    "- 它由大量的节点（神经元）组成，这些节点通过连接（突触）相互作用。\n",
    "- 神经网络通过调整连接权重来学习和适应输入数据，从而实现模式识别、分类和预测等任务。\n",
    "- 神经网络的结构通常包括输入层、隐藏层和输出层，每一层由多个神经元组成。\n",
    "- 神经网络在图像识别、自然语言处理和游戏等领域取得了显著的成功，成为现代人工智能的重要组成部分。"
   ],
   "id": "acd6a94f27d8d5e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 神经网络的基本结构\n",
    "- 神经网络由多个层次组成，主要包括输入层、隐藏层和输出层。\n",
    "- 输入层负责接收外部数据，将其传递给隐藏层进行处理。\n",
    "- 隐藏层包含多个神经元，这些神经元通过加权连接接收输入，并应用激活函数进行非线性变换。\n",
    "- 输出层将隐藏层的结果转换为最终输出，用于分类或回归任务。\n",
    "- 神经网络通过调整连接权重和偏置来学习数据中的模式，从而实现预测和决策。"
   ],
   "id": "c9e63c9774b90dc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 神经网络层 Neural Network Layers\n",
    "- 神经网络由多个层次组成，每一层包含若干神经元。\n",
    "- 输入层（Input Layer）负责接收外部数据，将其传递给隐藏层进行处理。\n",
    "    - 输入层又叫第0层\n",
    "    - 输入层是一个向量$\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$，表示输入特征\n",
    "- 隐藏层（Hidden Layer）位于输入层和输出层之间，负责对输入数据进行复杂的非线性变换。\n",
    "    - 隐藏层可以有多个，称作第1层、第2层等\n",
    "    - 每个隐藏层包含若干神经元，每个神经元接收前一层的输出作为输入，输出称作激活值\n",
    "    - 用上标$[i]$表示第$i$层。例如，$\\mathbf{a}^{[1]} = [a_1^{[1]}, a_2^{[1]}, \\ldots, a_m^{[1]}]^\\mathsf{T}$表示第1层的激活值，$\\mathbf{w}_j^{[i]}$表示第$i$层第$j$个神经元的权重向量元的权重向量\n",
    "- 输出层（Output Layer）负责生成最终的预测结果。\n",
    "    - 输出层的结构取决于具体任务，例如分类任务中输出层通常包含多个神经元，每个神经元对应一个类别\n",
    "    - ![](./assets/nnlayer.png)\n",
    "        - 这里的$g$表示Sigmoid函数，即$\\sigma(z)$\n",
    "- 注意：上下标别搞混了\n",
    "    - 例如：$\\mathbf{a}^{[2]}$是第3层的输入，$\\mathbf{a}^{[3]}$是第3层的输出，若第4个神经元是逻辑回归，则\n",
    "      $$a_4^{[3]} = \\sigma(\\mathbf{w}_4^{[3]} \\cdot a_4^{[2]} + b_4)$$\n",
    "- 综上，有公式：\n",
    "$$a_j^{[i]} = g(\\mathbf{w}_j^{[i]} \\cdot \\mathbf{a}^{[i-1]} + b_j^{[i]})$$\n",
    "规定$\\mathbf{a}^{[0]} = \\mathbf{x}$，即输入层的激活值就是输入特征"
   ],
   "id": "11e585b7b6c6b46c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 预测与前向传播\n",
    "- 神经网络通过前向传播（Forward Propagation）过程进行预测。\n",
    "- 在前向传播过程中，输入数据通过各层神经元进行加权求和，并应用激活函数生成输出。\n",
    "- 每个神经元的输出作为下一层的输入，直到最终输出层生成预测结果。\n",
    "- 分为两步：\n",
    "    - 加权求和（Weighted Sum）：计算每个神经元的加权输入，即前一层的输出与当前层的权重矩阵相乘，再加上偏置项。\n",
    "    $$\n",
    "      z_j^{[i]} = \\mathbf{w}_j^{[i]} \\cdot \\mathbf{a}^{[i-1]} + b_j^{[i]}\n",
    "    $$\n",
    "    - 激活函数（Activation Function）：对加权输入应用非线性激活函数，生成当前层的输出。\n",
    "    $$\n",
    "      a_j^{[i]} = g(z_j^{[i]})\n",
    "    $$\n",
    "- 通过不断调整权重和偏置，神经网络能够学习数据中的复杂模式，实现准确的预测。"
   ],
   "id": "83fb650deeed3685"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 构建一个简单的神经网络模型\n",
    "- 在TensorFlow中，可以使用Keras API来构建神经网络模型。\n",
    "- 下面是一个简单的神经网络模型示例：\n",
    "```python\n",
    "layer_1 = tf.keras.layers.Dense(units=3, activation='sigmoid')\n",
    "layer_2 = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "model = tf.keras.Sequential([layer_1, layer_2])\n",
    "```\n",
    "- 该模型包含两个全连接层（Dense Layer），每个层使用Sigmoid激活函数。\n",
    "- 可以通过`model.compile(...)`方法来配置模型的训练参数。\n",
    "- 使用`model.fit(x, y)`方法进行模型训练，传入训练数据和标签。\n",
    "- 训练完成后，可以使用`model.predict(x_new)`方法进行预测。"
   ],
   "id": "d6e9139dce58f779"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 单层前向传播\n",
    "- 对于上述的神经网络模型，可以手动实现单层的前向传播过程。\n",
    "```python\n",
    "x = np.array([200, 17])\n",
    "w1_1 = np.array([1, 2])\n",
    "b1_1 = np.array([-1])\n",
    "z1_1 = np.dot(w1_1, x) + b1_1\n",
    "a1_1 = sigmoid(z1_1)\n",
    "\n",
    "# continue for other neurons: a1_2, a1_3\n",
    "\n",
    "a1 = np.array([a1_1, a1_2, a1_3])\n",
    "```\n",
    "- 通用实现：定义一个`dense`函数，能够实现单层的前向传播。\n",
    "- 根据之前的分析，可以写成\n",
    "$$\n",
    "\\mathbf{a}^{[i]} = g(\\mathbf{a}^{[i-1]} \\cdot \\mathbf{W}^{[i]} + \\mathbf{b}^{[i]})\n",
    "$$\n",
    "其中$\\mathbf{a}^{[i]} \\in \\mathbb{R}^{1 \\times n}$，$\\mathbf{W}^{[i]} \\in \\mathbb{R}^{m \\times n}$，$\\mathbf{a}^{[i-1]} \\in \\mathbb{R}^{1 \\times m}$，$\\mathbf{b}^{[i]} \\in \\mathbb{R}^{1 \\times n}$\n",
    "\n",
    "- 可以实现如下：\n",
    "```python\n",
    "def dense(a_prev, W, b):\n",
    "    z = np.matmul(a_prev, W) + b\n",
    "    a_out = g(z)\n",
    "    return a_out\n",
    "```"
   ],
   "id": "5ba8aab0a7014e42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型训练\n",
    "- TensorFlow中的Keras API提供了便捷的模型训练方法。\n",
    "```python\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "model.compile(loss=BinaryCrossentropy())\n",
    "model.fit(x, y, epochs=1000)\n",
    "```\n",
    "- 通过`model.compile(...)`方法配置损失函数、优化器和评估指标。\n",
    "- 使用`model.fit(x, y, epochs=1000)`方法进行模型训练，传入训练数据和标签，并指定训练轮数。\n",
    "## 训练的细节步骤：\n",
    "1. 定义模型，确定如何计算$f(\\mathbf{x})$\n",
    "    - 通过`model = tf.keras.Sequential([...])`定义模型结构\n",
    "2. 定义损失函数和成本函数，确定如何衡量预测值与真实值\n",
    "    - 通过`model.compile(loss=...)`定义损失函数\n",
    "    - 例如，二分类任务中常用的损失函数是二元交叉熵（Binary Crossentropy）；线性回归任务中常用的损失函数是均方误差（Mean Squared Error）\n",
    "```python\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# 二分类任务\n",
    "model.compile(loss=BinaryCrossentropy())\n",
    "\n",
    "# 线性回归任务\n",
    "model.compile(loss=MeanSquaredError())\n",
    "```\n",
    "3. 训练模型，调整参数以最小化成本函数\n",
    "    - 通过`model.fit(x, y, epochs=...)`进行训练\n",
    "    - 实现的细节是采用反向传播算法（Backpropagation）和梯度下降法（Gradient Descent）来更新模型参数"
   ],
   "id": "8765128c056f4d89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 常用的激活函数类型\n",
    "1. ReLU（Rectified Linear Unit）\n",
    "    - 公式：$g(z) = \\max(0, z)$\n",
    "    - 适用于$y$取非负实数的回归任务\n",
    "    - 收敛速度比Sigmoid函数更快\n",
    "2. 线性激活函数（Linear Activation Function）\n",
    "    - 公式：$g(z) = z$\n",
    "    - 适用于$y$取正或负实数的回归任务\n",
    "3. Sigmoid函数（Sigmoid Function）\n",
    "    - 公式：$g(z) = \\frac{1}{1 + \\mathrm{e}^{-z}}$\n",
    "    - 适用于二分类任务，输出范围为$\\{0, 1\\}$\n",
    "在隐藏层，通常使用ReLU函数作为激活函数，而在输出层，根据具体任务选择合适的激活函数，例如二分类任务中使用Sigmoid函数，回归任务中使用线性激活函数。\n",
    "## 为什么要用激活函数？\n",
    "- 激活函数引入非线性，使神经网络能够学习和表示复杂的非线性关系。\n",
    "- 如果没有激活函数，神经网络将退化为线性模型，无法处理复杂的任务。\n",
    "- 可以证明：如果神经网络的隐藏层中全是线性激活函数，那么无论有多少层，整体仍然是一个线性变换；如果在隐藏层中引入非线性激活函数，则可以表示更复杂的函数。通常我们使用ReLU作为隐藏层的激活函数。"
   ],
   "id": "b32fd279bb6b1b50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 多分类任务与Softmax\n",
    "- 多分类任务是指将输入数据分为多个类别的任务（不是无限个类别）。例如手写数字识别任务中，将输入图像分为0-9共10个类别。\n",
    "- Softmax函数是一种常用的多分类激活函数，用于将神经网络的输出转换为概率分布。\n",
    "- 假设神经网络的原始输出为$\\mathbf{z} = [z_1, z_2, \\ldots, z_K]$，其中$K$是类别数，则Softmax函数定义为：\n",
    "$$\n",
    "a_j = \\frac{\\mathrm{e}^{z_j}}{\\sum\\limits_{i=1}^{K} \\mathrm{e}^{z_i}} = P(y=j|\\mathbf{x})\n",
    "$$\n",
    "- Softmax函数的输出可以看作是每个类别的预测概率，所有类别的概率之和为1。即：\n",
    "$$\\sum_{j=1}^{K} a_j = 1\n",
    "$$\n",
    "- Softmax的损失函数通常使用交叉熵损失（Cross-Entropy Loss），即：\n",
    "$$\n",
    "L = -\\log(a_{j}),\\, \\text{if}\\, y=j\n",
    "$$\n",
    "- 带有Softmax激活函数的神经网络模型\n",
    "    - 在最后一层：\n",
    "$$\n",
    "z_j^{[L]} = \\mathbf{w}_j^{[L]} \\cdot \\mathbf{a}^{[L-1]} + b_j^{[L]}\n",
    "$$\n",
    "$$\n",
    "a_j^{[L]} = \\frac{\\mathrm{e}^{z_j^{[L]}}}{\\sum\\limits_{i=1}^{K} \\mathrm{e}^{z_i^{[L]}}}\n",
    "$$\n",
    "    - 输出层使用Softmax激活函数，每个$a_j^{[L]}$需要由所有$z_i^{[L]}$共同计算得到\n",
    "$$\n",
    "a_j^{[L]} = g(z_1^{[L]}, z_2^{[L]}, \\ldots, z_K^{[L]}) = \\frac{\\mathrm{e}^{z_j^{[L]}}}{\\sum\\limits_{i=1}^{K} \\mathrm{e}^{z_i^{[L]}}}\n",
    "$$\n",
    "    - 这与单变量激活函数不同，Softmax是一个多变量函数。\n",
    "- 使用TensorFlow中的Keras API构建多分类神经网络模型\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "model.compile(loss=SparseCategoricalCrossentropy())\n",
    "```\n",
    "- 改进的实现：\n",
    "    - `from_logits=True`参数可以让损失函数自动处理Softmax计算，避免数值不稳定问题\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=10, activation='linear')  # No activation here\n",
    "])\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True))\n",
    "```"
   ],
   "id": "2752a11be951120"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-label Classification 多标签分类\n",
    "- 多标签分类任务是指每个输入样本可以同时属于多个类别的任务。\n",
    "    - 例如，在图像标注任务中，一张图片可能同时包含“猫”和“沙发”两个标签。\n",
    "- 与多分类任务不同，多标签分类任务的输出层通常使用Sigmoid激活函数，而不是Softmax函数。\n",
    "    - 这是因为每个类别的预测是独立的，可以同时为多个类别输出正类概率。\n",
    "- 假设神经网络的原始输出为$\\mathbf{z} = [z_1, z_2, \\ldots, z_K]$，则最终的输出为：\n",
    "$$\n",
    "a_j = \\sigma(z_j) = \\frac{1}{1 + \\mathrm{e}^{-z_j}}\n",
    "$$"
   ],
   "id": "a8b3476ae948b3fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "cd58fc0efe2a1362"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
