{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 分类 Classification",
   "id": "440a43735e076230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Motivation\n",
    "- 分类是机器学习中的一种监督学习任务，旨在将输入数据分配到预定义的类别或标签中。分类任务广泛应用于各种领域，如垃圾邮件检测、图像识别、情感分析等。\n",
    "- 但是线性回归无法很好地处理分类问题，因为它可能会输出超出0和1范围的值，这在概率解释上是不合理的。因此，我们需要使用专门的分类算法来解决这些问题。"
   ],
   "id": "2b68250877c7705a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 逻辑回归 Logistic Regression\n",
    "- 逻辑回归是一种广泛使用的分类算法，适用于二分类。\n",
    "- 逻辑函数（Logistic Function）或称为Sigmoid函数，将线性组合的输入映射到0和1之间的概率值。\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + \\mathrm{e}^{-z}}\n",
    "$$\n",
    "- 显然：\n",
    "    - 当 $ z \\to +\\infty $ 时，$ \\sigma(z) \\to 1 $\n",
    "    - 当 $z \\to -\\infty$ 时，$\\sigma(z) \\to 0$\n",
    "    - 这样就保证了$\\sigma(z)$ 的输出在0和1之间。\n",
    "- 构建逻辑回归模型：\n",
    "    - 首先，我们计算线性组合：\n",
    "    $$\n",
    "    z = \\mathbf{w}^\\mathsf{T} \\mathbf{x} + b\n",
    "    $$\n",
    "    - 然后，将其输入到逻辑函数中，得到逻辑回归表达式：\n",
    "    $$\n",
    "    \\hat{y} = f_{\\mathbf{w},b}(\\mathbf{x}) = \\sigma(z) = \\frac{1}{1 + \\mathrm{e}^{-(\\mathbf{w}^\\mathsf{T} \\mathbf{x} + b)}}\n",
    "    $$\n",
    "- 对$f_{\\mathbf{w},b}(\\mathbf{x})$的理解：概率\n",
    "    - 逻辑回归模型的输出$\\hat{y}$可以解释为输入样本$\\mathbf{x}$属于正类（标签为1）的概率，即：\n",
    "    $$\n",
    "    \\hat{y} = P(y=1|\\mathbf{x}; \\mathbf{w}, b)\n",
    "    $$"
   ],
   "id": "1deaffac6b682ad8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 决策边界 Decision Boundary\n",
    "- 在逻辑回归中，决策边界是将不同类别分开的边界。\n",
    "- 对于二分类问题，通常选择0.5作为阈值来决定类别：\n",
    "    - 如果$\\hat{y} \\geq 0.5$，则预测类别为1（正类）。\n",
    "    - 如果$\\hat{y} < 0.5$，则预测类别为0（负类）。\n",
    "- 当$\\hat{y} \\geq 0.5$时，此时有：\n",
    "$$\n",
    "z = \\mathbf{w}^\\mathsf{T} \\mathbf{x} + b \\geq 0\n",
    "$$\n",
    "- 反之：\n",
    "$$\n",
    "\\hat{y}<0.5 \\Leftrightarrow z = \\mathbf{w}^\\mathsf{T} \\mathbf{x} + b < 0\n",
    "$$\n",
    "- 因此，决策边界可以通过以下方程表示：\n",
    "$$\n",
    "z =\\mathbf{w}^\\mathsf{T} \\mathbf{x} + b= 0\n",
    "$$\n",
    "- 这个方程定义了一个超平面，将特征空间划分为两个区域，每个区域对应一个类别。\n",
    "- 对于$z$不是线性函数的分类模型，决策边界的形状可能更加复杂，但决策边界仍然是$z=0$。\n",
    "- 由于逻辑回归的决策边界可以是多种形式的，因此它可以适应更复杂的数据分布情况。"
   ],
   "id": "abf46ad0f6bf892e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 损失函数 Loss Function\n",
    "- 由于逻辑回归的逻辑函数不是线性的，有很多的局部极小值，在逻辑回归中，我们使用对数损失函数（Log Loss）来衡量模型预测与实际标签之间的差异。\n",
    "- 对数损失函数定义如下：\n",
    "$$\n",
    "L(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}),y^{(i)}) = \\begin{cases}\n",
    "-\\log(\\hat{y}^{(i)}) = -\\log(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) & \\text{if } y^{(i)}=1 \\\\\n",
    "-\\log(1 - \\hat{y}^{(i)}) = -\\log(1-f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) & \\text{if } y^{(i)}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "- 其中，$\\hat{y}^{(i)} = f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$是第$i$组样例的预测概率，$y^{(i)}$是实际标签（0或1）。\n",
    "- 损失函数的意义："
   ],
   "id": "b507f2054b73d454"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
