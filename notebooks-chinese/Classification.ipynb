{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 分类 Classification",
   "id": "440a43735e076230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Motivation\n",
    "- 分类是机器学习中的一种监督学习任务，旨在将输入数据分配到预定义的类别或标签中。分类任务广泛应用于各种领域，如垃圾邮件检测、图像识别、情感分析等。\n",
    "- 但是线性回归无法很好地处理分类问题，因为它可能会输出超出0和1范围的值，这在概率解释上是不合理的。因此，我们需要使用专门的分类算法来解决这些问题。"
   ],
   "id": "2b68250877c7705a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 逻辑回归 Logistic Regression\n",
    "- 逻辑回归是一种广泛使用的分类算法，适用于二分类。\n",
    "- 逻辑函数（Logistic Function）或称为Sigmoid函数，将线性组合的输入映射到0和1之间的概率值。\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + \\mathrm{e}^{-z}}\n",
    "$$\n",
    "- 显然：\n",
    "    - 当 $ z \\to +\\infty $ 时，$ \\sigma(z) \\to 1 $\n",
    "    - 当 $z \\to -\\infty$ 时，$\\sigma(z) \\to 0$\n",
    "    - 这样就保证了$\\sigma(z)$ 的输出在0和1之间。\n",
    "- 构建逻辑回归模型：\n",
    "    - 首先，我们计算线性组合：\n",
    "    $$\n",
    "    z = \\mathbf{w}^\\mathsf{T} \\mathbf{x} + b\n",
    "    $$\n",
    "    - 然后，将其输入到逻辑函数中，得到逻辑回归表达式：\n",
    "    $$\n",
    "    \\hat{y} = f_{\\mathbf{w},b}(\\mathbf{x}) = \\sigma(z) = \\frac{1}{1 + \\mathrm{e}^{-(\\mathbf{w}^\\mathsf{T} \\mathbf{x} + b)}}\n",
    "    $$\n",
    "- 对$f_{\\mathbf{w},b}(\\mathbf{x})$的理解：概率\n",
    "    - 逻辑回归模型的输出$\\hat{y}$可以解释为输入样本$\\mathbf{x}$属于正类（标签为1）的概率，即：\n",
    "    $$\n",
    "    \\hat{y} = P(y=1|\\mathbf{x}; \\mathbf{w}, b)\n",
    "    $$"
   ],
   "id": "1deaffac6b682ad8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 决策边界 Decision Boundary\n",
    "- 在逻辑回归中，决策边界是将不同类别分开的边界。\n",
    "- 对于二分类问题，通常选择0.5作为阈值来决定类别：\n",
    "    - 如果$\\hat{y} \\geq 0.5$，则预测类别为1（正类）。\n",
    "    - 如果$\\hat{y} < 0.5$，则预测类别为0（负类）。\n",
    "- 当$\\hat{y} \\geq 0.5$时，此时有：\n",
    "$$\n",
    "z = \\mathbf{w}^\\mathsf{T} \\mathbf{x} + b \\geq 0\n",
    "$$\n",
    "- 反之：\n",
    "$$\n",
    "\\hat{y}<0.5 \\Leftrightarrow z = \\mathbf{w}^\\mathsf{T} \\mathbf{x} + b < 0\n",
    "$$\n",
    "- 因此，决策边界可以通过以下方程表示：\n",
    "$$\n",
    "z =\\mathbf{w}^\\mathsf{T} \\mathbf{x} + b= 0\n",
    "$$\n",
    "- 这个方程定义了一个超平面，将特征空间划分为两个区域，每个区域对应一个类别。\n",
    "- 对于$z$不是线性函数的分类模型，决策边界的形状可能更加复杂，但决策边界仍然是$z=0$。\n",
    "- 由于逻辑回归的决策边界可以是多种形式的，因此它可以适应更复杂的数据分布情况。"
   ],
   "id": "abf46ad0f6bf892e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 损失函数 Loss Function\n",
    "- 由于逻辑回归的逻辑函数不是线性的，有很多的局部极小值，在逻辑回归中，我们使用对数损失函数（Log Loss）来衡量模型预测与实际标签之间的差异。\n",
    "- 对数损失函数定义如下：\n",
    "$$\n",
    "L(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}),y^{(i)}) = \\begin{cases}\n",
    "-\\log(\\hat{y}^{(i)}) = -\\log(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) & \\text{if } y^{(i)}=1 \\\\\n",
    "-\\log(1 - \\hat{y}^{(i)}) = -\\log(1-f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) & \\text{if } y^{(i)}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "- 其中，$\\hat{y}^{(i)} = f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$是第$i$组样例的预测概率，$y^{(i)}$是实际标签（0或1）。\n",
    "- 损失函数的理解：\n",
    "    - 当实际标签为1时，如果模型预测的概率$\\hat{y}^{(i)}$接近1，损失值接近0；如果$\\hat{y}^{(i)}$接近0，损失值趋向于无穷大。\n",
    "    - 当实际标签为0时，如果模型预测的概率$\\hat{y}^{(i)}$接近0，损失值接近0；如果$\\hat{y}^{(i)}$接近1，损失值趋向于无穷大。\n",
    "- 逻辑回归的损失函数也是凸函数，这使得我们可以使用梯度下降等优化算法来找到全局最优解。\n",
    "\n",
    "## 简化的损失函数\n",
    "- 逻辑回归的损失函数可以简化为一个统一的表达式：\n",
    "$$\n",
    "L(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}),y^{(i)}) = - \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
    "$$\n",
    "- 这个表达式在实际标签为1和0时都适用，简化了计算过程。\n",
    "## 成本函数 Cost Function\n",
    "- 成本函数用于衡量整个训练集上的平均损失，根据概率论的最大似然估计原理，逻辑回归的成本函数定义为：\n",
    "$$\n",
    "J(\\mathbf{w}, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}),y^{(i)})\n",
    "$$\n",
    "也就是：\n",
    "$$\n",
    "J(\\mathbf{w}, b) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) + (1 - y^{(i)}) \\log(1 - f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})) \\right]\n",
    "$$\n",
    "- 其中，$m$是训练样本的数量，$L(f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}),y^{(i)})$是第$i$组样例的损失值。\n",
    "- 成本函数的目标是最小化，通过调整模型参数$\\mathbf{w}$和$b$来提高模型的预测性能。"
   ],
   "id": "b507f2054b73d454"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 梯度下降 Gradient Descent\n",
    "- 为了最小化成本函数$J(\\mathbf{w}, b)$，我们可以使用梯度下降算法。\n",
    "- 梯度下降的更新规则如下：\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial w_j} = w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J(\\mathbf{w}, b)}{\\partial b} = b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)}) - y^{(i)} \\right)\n",
    "$$\n",
    "- 可以发现，逻辑回归的梯度下降更新规则与线性回归非常相似，唯一的区别在于预测函数$f_{\\mathbf{w}, b}(\\mathbf{x}^{(i)})$不同。"
   ],
   "id": "cde13da097325d73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 使用Sklearn实现逻辑回归\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 生成一个二分类数据集\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "# 将数据集分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# 创建逻辑回归模型\n",
    "model = LogisticRegression()\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train)\n",
    "# 进行预测\n",
    "y_pred = model.predict(X_test)\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "# 混淆矩阵\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "# 分类报告\n",
    "report = classification_report(y_test, y_pred)\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "# 可视化决策边界\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ],
   "id": "9539a655977a306a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 过拟合 Overfitting\n",
    "- 拟合的三种情况：\n",
    "    - 欠拟合（Underfitting），高偏差（High Bias）：模型过于简单，无法捕捉数据的复杂模式，导致训练误差和测试误差都较高。\n",
    "    - 适当拟合（Good Fit）：模型能够很好地捕捉数据的模式，训练误差和测试误差都较低。\n",
    "    - 过拟合（Overfitting），高方差（High Variance）：模型过于复杂，过度拟合训练数据中的噪声，导致训练误差较低但测试误差较高。\n",
    "    - ![](./assets/overfitting.png)\n",
    "- 如何解决过拟合问题：\n",
    "    - 增加训练数据量：更多的数据可以帮助模型更好地学习数据的真实模式。\n",
    "    - 特征选择：选择更少的特征，去除不相关或冗余的特征，减少模型的复杂度。\n",
    "    - 正则化（Regularization）：对模型参数施加惩罚，防止参数过大，从而减少模型的复杂度。"
   ],
   "id": "db1a8aca68227113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 正则化 Regularization\n",
    "- 正则化是一种防止过拟合的技术，通过在损失函数中添加一个惩罚项来限制模型的复杂度。\n",
    "- 常见的正则化方法有L1正则化和L2正则化。\n",
    "- L2正则化（Ridge Regression）：\n",
    "    - 在成本函数中添加L2范数惩罚项。例如下面的均方根误差损失函数：\n",
    "$$\n",
    "    J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "    - 其中，$\\lambda$是正则化参数（Regularization Parameter），控制惩罚项的强度。\n",
    "- L1正则化（Lasso Regression）：\n",
    "    - 在成本函数中添加L1范数惩罚项。例如下面的均方根误差损失函数：\n",
    "$$\n",
    "    J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} |w_j|\n",
    "$$"
   ],
   "id": "e5bc72f04a18f2bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
